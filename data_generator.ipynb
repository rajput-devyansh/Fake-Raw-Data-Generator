{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "727e7b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from faker import Faker\n",
    "import random\n",
    "import os\n",
    "import datetime\n",
    "from src.Generic_Errors import apply_generic_chaos\n",
    "from src.Format_Errors import chaotic_date\n",
    "from src.Format_Errors import chaotic_case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5cf3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURATION\n",
    "DATASET_SIZE = 500  # Number of rows per file\n",
    "OUTPUT_DIR = \"Generated_Data\"\n",
    "\n",
    "fake = Faker()\n",
    "Faker.seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "rows = DATASET_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d90bd55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Retail Dataset with 10000 rows.\n",
      "Retail Dataset generated and saved.\n"
     ]
    }
   ],
   "source": [
    "# Retail Dataset \n",
    "print(f\"Generating Retail Dataset with {rows} rows.\")\n",
    "\n",
    "# Setup Lists\n",
    "categories = ['Electronics', 'Clothing', 'Home', 'Grocery', 'Toys', 'Books']\n",
    "payment_methods = ['Credit Card', 'Cash', 'UPI', 'Debit Card', 'Wallet']\n",
    "\n",
    "df_ret = pd.DataFrame({\n",
    "    \"Trans_ID\": [fake.uuid4()[:8] for _ in range(rows)],\n",
    "    \"Date\": [fake.date_this_year() for _ in range(rows)],\n",
    "    \"Customer_Name\": [fake.name() for _ in range(rows)],\n",
    "    \"Customer_Email\": [fake.email() for _ in range(rows)],\n",
    "    \"Store_City\": [fake.city() for _ in range(rows)],\n",
    "    \"Payment_Method\": [random.choice(payment_methods) for _ in range(rows)],\n",
    "    \"Is_Member\": [random.choice([True, False]) for _ in range(rows)],\n",
    "    \"Discount_Pct\": [random.choice([0, 0, 0.05, 0.10, 0.15, 0.20, 0.50]) for _ in range(rows)],\n",
    "    \"Category\": [random.choice(categories) for _ in range(rows)],\n",
    "    \"Qty\": [random.choice([1, 2, 3, 5, 10, 0]) for _ in range(rows)],\n",
    "    \"Price\": [round(random.uniform(10, 500), 2) for _ in range(rows)]\n",
    "})\n",
    "\n",
    "# Calculate Total with Discount applied\n",
    "df_ret[\"Total\"] = (df_ret[\"Price\"] * df_ret[\"Qty\"]) * (1 - df_ret[\"Discount_Pct\"])\n",
    "df_ret[\"Total\"] = df_ret[\"Total\"].round(2)\n",
    "\n",
    "# 1. Existing Chaos\n",
    "df_ret.loc[0:50, \"Total\"] += 100 \n",
    "df_ret.loc[51:100, \"Qty\"] = -5 \n",
    "df_ret.loc[101:150, \"Date\"] = datetime.date(2099, 1, 1) \n",
    "\n",
    "# 2. New Chaos for new columns\n",
    "# Make some discounts greater than 100% (Math error testing)\n",
    "df_ret.loc[151:170, \"Discount_Pct\"] = 1.50 \n",
    "\n",
    "# Make some Payment Methods NaN/Null\n",
    "df_ret.loc[171:200, \"Payment_Method\"] = None\n",
    "\n",
    "# Corrupt some email formats\n",
    "df_ret.loc[201:220, \"Customer_Email\"] = \"user_at_gmail.com\" # Missing @\n",
    "\n",
    "# 3. Apply Styling Chaos\n",
    "df_ret[\"Category\"] = df_ret[\"Category\"].apply(chaotic_case) \n",
    "df_ret[\"Date\"] = df_ret[\"Date\"].apply(chaotic_date) \n",
    "df_ret = apply_generic_chaos(df_ret)\n",
    "\n",
    "# Export\n",
    "df_ret.to_csv(f\"{OUTPUT_DIR}/Retail_dataset.csv\", index=False)\n",
    "print(\"Retail Dataset generated and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efdd1f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Finance Dataset with 10000 rows.\n",
      "Finance Dataset generated and saved.\n"
     ]
    }
   ],
   "source": [
    "# Finance Dataset\n",
    "print(f\"Generating Finance Dataset with {rows} rows.\")\n",
    "\n",
    "# Setup Lists\n",
    "loan_purposes = ['Debt Consolidation', 'Home Improvement', 'Business', 'Education', 'Car']\n",
    "emp_lengths = ['< 1 year', '1-3 years', '4-7 years', '10+ years', 'Unemployed']\n",
    "\n",
    "df_fin = pd.DataFrame({\n",
    "    \"App_ID\": [f\"LN-{random.randint(1000,9999)}\" for _ in range(rows)],\n",
    "    \"App_Date\": [fake.date_this_year() for _ in range(rows)],\n",
    "    \"Applicant_Name\": [fake.name() for _ in range(rows)],\n",
    "    \"State\": [fake.state_abbr() for _ in range(rows)],\n",
    "    \"Employment_Length\": [random.choice(emp_lengths) for _ in range(rows)],\n",
    "    \"Annual_Income\": [random.randint(-5000, 150000) for _ in range(rows)], # Includes negative income\n",
    "    \"Loan_Amount\": [random.randint(1000, 50000) for _ in range(rows)],\n",
    "    \"Loan_Term_Months\": [random.choice([12, 24, 36, 60, 360, 999]) for _ in range(rows)],\n",
    "    \"Interest_Rate\": [round(random.uniform(2.5, 35.0), 2) for _ in range(rows)],\n",
    "    \"Purpose\": [random.choice(loan_purposes) for _ in range(rows)],\n",
    "    \"Credit_Score\": [random.randint(300, 950) for _ in range(rows)],\n",
    "    \"Current_Debt\": [random.randint(0, 50000) for _ in range(rows)],\n",
    "    \"Status\": [random.choice(['Approved', 'Rejected', 'Pending']) for _ in range(rows)]\n",
    "})\n",
    "\n",
    "# 1. Zero Division Chaos: Force some Income to 0 to create 'inf' (Infinity) in DTI\n",
    "df_fin.loc[0:20, \"Annual_Income\"] = 0\n",
    "\n",
    "# 2. Calculate DTI (Debt to Income)\n",
    "# In Pandas, dividing by zero results in 'inf', which is excellent for testing.\n",
    "df_fin[\"DTI_Ratio\"] = df_fin[\"Current_Debt\"] / df_fin[\"Annual_Income\"]\n",
    "\n",
    "# 1. Logic Mismatch: High Credit Score (800+) but Rejected\n",
    "good_credit_indices = df_fin[df_fin[\"Credit_Score\"] > 800].index\n",
    "df_fin.loc[good_credit_indices[:10], \"Status\"] = \"Rejected\"\n",
    "\n",
    "# 2. Logic Mismatch: Low Credit Score (<500) but Approved\n",
    "bad_credit_indices = df_fin[df_fin[\"Credit_Score\"] < 500].index\n",
    "df_fin.loc[bad_credit_indices[:10], \"Status\"] = \"Approved\"\n",
    "\n",
    "# 3. Impossible Values: Negative Interest Rates\n",
    "df_fin.loc[50:60, \"Interest_Rate\"] = -5.00\n",
    "\n",
    "# 4. Data Type Chaos: String in Numeric Column (\"TBD\" in Loan Amount)\n",
    "df_fin = df_fin.astype(object) # Ensure we can mix types safely\n",
    "df_fin.loc[61:70, \"Loan_Amount\"] = \"TBD\"\n",
    "\n",
    "# Apply the Generic Chaos (Typos, Nulls, Encoding)\n",
    "df_fin = apply_generic_chaos(df_fin)\n",
    "\n",
    "df_fin.to_csv(f\"{OUTPUT_DIR}/Finance_dataset.csv\", index=False)\n",
    "print(\"Finance Dataset generated and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f607b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Supply Chain Dataset with 10000 rows.\n",
      "Supply Chain Dataset generated and saved.\n"
     ]
    }
   ],
   "source": [
    "# Supply Chain Dataset\n",
    "print(f\"Generating Supply Chain Dataset with {rows} rows.\")\n",
    "\n",
    "carriers = ['Maersk', 'DHL', 'FedEx', 'UPS', 'Old Dominion', 'Generic_Trucking']\n",
    "statuses = ['In Transit', 'Delivered', 'Delayed', 'Customs Hold', 'Lost']\n",
    "products = ['Electronics', 'Perishables', 'Raw Materials', 'Hazmat', 'Textiles']\n",
    "\n",
    "df_sc = pd.DataFrame({\n",
    "    \"Shipment_ID\": [fake.bothify('??-####-####') for _ in range(rows)],\n",
    "    \"SKU\": [fake.ean13() for _ in range(rows)],\n",
    "    \"Origin\": [fake.city() for _ in range(rows)],\n",
    "    \"Destination\": [random.choice([fake.city(), fake.state(), \"USA\", \"India\", \"Unknown\"]) for _ in range(rows)],\n",
    "    \n",
    "    # --- NEW COLUMNS ---\n",
    "    \"Carrier\": [random.choice(carriers) for _ in range(rows)],\n",
    "    \"Product_Type\": [random.choice(products) for _ in range(rows)],\n",
    "    \"Shipping_Cost\": [round(random.uniform(50, 5000), 2) for _ in range(rows)],\n",
    "    \"Temperature_C\": [round(random.uniform(-30, 30), 1) for _ in range(rows)], # For cold chain testing\n",
    "    \"Fragile\": [random.choice([True, False, \"Yes\", \"No\", 1, 0]) for _ in range(rows)], # Mixed boolean types\n",
    "    # -------------------\n",
    "\n",
    "    # Weight: Mixed units logic (kg, lbs, or no unit)\n",
    "    \"Weight\": [f\"{random.randint(10,500)}{random.choice(['kg', ' kg', 'lbs', 'Lbs', ''])}\" for _ in range(rows)],\n",
    "    \n",
    "    \"Ship_Date\": [fake.date_this_year() for _ in range(rows)],\n",
    "    \"Arrival_Date\": [fake.date_this_year() for _ in range(rows)],\n",
    "    \"Status\": [random.choice(statuses) for _ in range(rows)]\n",
    "})\n",
    "\n",
    "# --- LOGIC & CHAOS UPDATES ---\n",
    "\n",
    "# 1. Time Travel (Arrival before Shipping)\n",
    "# Move logic to indexes 0-20\n",
    "df_sc.loc[0:20, \"Arrival_Date\"] = df_sc.loc[0:20, \"Ship_Date\"] - datetime.timedelta(days=10)\n",
    "\n",
    "# 2. Cold Chain Failure (Perishables that got too hot)\n",
    "# Find Perishables and set temp to > 20C (Spoiled)\n",
    "mask_hot = (df_sc['Product_Type'] == 'Perishables')\n",
    "df_sc.loc[mask_hot, \"Temperature_C\"] = df_sc.loc[mask_hot, \"Temperature_C\"].apply(lambda x: random.uniform(20, 40))\n",
    "\n",
    "# 3. Negative Shipping Costs (Billing Error)\n",
    "df_sc.loc[21:30, \"Shipping_Cost\"] = -150.00\n",
    "\n",
    "# 4. Ghost Shipments (Status = Delivered, but Arrival_Date is Null)\n",
    "df_sc.loc[31:50, \"Status\"] = \"Delivered\"\n",
    "df_sc.loc[31:50, \"Arrival_Date\"] = np.nan\n",
    "\n",
    "# 5. Apply Generic Chaos (Typos, Encoding, etc.)\n",
    "# We cast to object first to ensure safety, as per previous fixes\n",
    "df_sc = df_sc.astype(object)\n",
    "df_sc = apply_generic_chaos(df_sc)\n",
    "\n",
    "# --- EXPORT & MANUAL INJECTION ---\n",
    "\n",
    "# Step 1: Write the main CSV first (mode='w' creates the file)\n",
    "csv_path = f\"{OUTPUT_DIR}/Supply_chain_dataset.csv\"\n",
    "df_sc.to_csv(csv_path, index=False)\n",
    "\n",
    "# Step 2: Append the \"Bad Row\" (mode='a' adds to the end)\n",
    "# This simulates a file transfer error or a jagged row (missing columns)\n",
    "with open(csv_path, \"a\") as f:\n",
    "    f.write(\"\\nBAD_ROW_DATA,New York, NY,50kg,2023-01-01\") # Deliberately missing columns\n",
    "    f.write(\"\\nANOTHER_BAD_ROW\" + \",\" * 15) # Row with way too many commas\n",
    "\n",
    "print(\"Supply Chain Dataset generated and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5c78de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Healthcare Dataset with 10000 rows.\n",
      "Healthcare Dataset generated and saved.\n"
     ]
    }
   ],
   "source": [
    "# Healthcare Dataset\n",
    "print(f\"Generating Healthcare Dataset with {rows} rows.\")\n",
    "\n",
    "diagnoses = ['Hypertension', 'Diabetes Type 2', 'Fracture', 'Viral Infection', 'COVID-19', 'Migraine']\n",
    "blood_types = ['A+', 'A-', 'B+', 'B-', 'O+', 'O-', 'AB+', 'AB-']\n",
    "insurance_providers = ['BlueCross', 'Aetna', 'Medicare', 'Private', 'Uninsured']\n",
    "\n",
    "df_hlth = pd.DataFrame({\n",
    "    \"Patient_ID\": [f\"P-{random.randint(10000, 99999)}\" for _ in range(rows)],\n",
    "    \n",
    "    # --- NEW COLUMNS ---\n",
    "    \"Patient_Name\": [fake.name() for _ in range(rows)], # PII for de-anonymization testing\n",
    "    \"SSN_Last4\": [f\"{random.randint(1000,9999)}\" for _ in range(rows)],\n",
    "    \"Blood_Type\": [random.choice(blood_types) for _ in range(rows)],\n",
    "    \"Diagnosis\": [random.choice(diagnoses) for _ in range(rows)],\n",
    "    \"Bill_Amount\": [round(random.uniform(100.0, 50000.0), 2) for _ in range(rows)],\n",
    "    \"Insurance\": [random.choice(insurance_providers) for _ in range(rows)],\n",
    "    \n",
    "    # Blood Pressure: mostly valid \"120/80\" format, but we will mess this up later\n",
    "    \"Blood_Pressure\": [f\"{random.randint(90,160)}/{random.randint(60,100)}\" for _ in range(rows)],\n",
    "    # -------------------\n",
    "\n",
    "    # Age: Keep your wide range, we will add negative ages in chaos\n",
    "    \"Age\": [random.randint(0, 110) for _ in range(rows)], \n",
    "    \n",
    "    # Gender: Keep your messy formatting (great for cleaning tests)\n",
    "    \"Gender\": [random.choice(['M', 'F', 'Male', 'Female', 'm', 'f', 'NB', 'Other']) for _ in range(rows)], \n",
    "    \n",
    "    \"Admit_Date\": [fake.date_this_year() for _ in range(rows)],\n",
    "    \"Discharge_Date\": [fake.date_this_year() for _ in range(rows)]\n",
    "})\n",
    "\n",
    "# --- LOGIC & CHAOS UPDATES ---\n",
    "\n",
    "# 1. Impossible Dates (Discharge BEFORE Admit) - 5% of rows\n",
    "bad_dates_mask = np.random.rand(len(df_hlth)) < 0.05\n",
    "df_hlth.loc[bad_dates_mask, \"Discharge_Date\"] = df_hlth.loc[bad_dates_mask, \"Admit_Date\"] - datetime.timedelta(days=5)\n",
    "\n",
    "# 2. Impossible Data (Negative Age)\n",
    "df_hlth.loc[0:10, \"Age\"] = [random.randint(-50, -1) for _ in range(11)]\n",
    "\n",
    "# 3. Dirty Data (Blood Pressure Format)\n",
    "# Mix in different separators or just text\n",
    "df_hlth.loc[11:20, \"Blood_Pressure\"] = \"High\" \n",
    "df_hlth.loc[21:30, \"Blood_Pressure\"] = \"140-90\" # Wrong separator\n",
    "df_hlth.loc[31:40, \"Blood_Pressure\"] = \"120 / 80\" # Extra spaces\n",
    "\n",
    "# 4. Mixed Types in Billing (Currency Symbols)\n",
    "# We cast to object first to allow string injection into float column\n",
    "df_hlth = df_hlth.astype(object) \n",
    "df_hlth.loc[41:50, \"Bill_Amount\"] = \"$500.00\"\n",
    "\n",
    "# 5. Apply Generic Chaos\n",
    "df_hlth = apply_generic_chaos(df_hlth)\n",
    "\n",
    "df_hlth.to_csv(f\"{OUTPUT_DIR}/Healthcare_dataset.csv\", index=False)\n",
    "print(\"Healthcare Dataset generated and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9124ba34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Marketing Dataset with 10000 rows.\n",
      "Marketing Dataset generated and saved.\n"
     ]
    }
   ],
   "source": [
    "# Marketing Dataset\n",
    "print(f\"Generating Marketing Dataset with {rows} rows.\")\n",
    "\n",
    "platforms = ['Facebook', 'Google Ads', 'TikTok', 'Email', 'LinkedIn', 'Instagram']\n",
    "ad_types = ['Video', 'Carousel', 'Static Image', 'Story', 'Reel']\n",
    "\n",
    "df_mkt = pd.DataFrame({\n",
    "    \"Campaign_ID\": [fake.uuid4()[:12] for _ in range(rows)],\n",
    "    \"Date\": [fake.date_this_year() for _ in range(rows)],\n",
    "    \n",
    "    # --- NEW COLUMNS ---\n",
    "    \"Channel\": [random.choice(platforms) for _ in range(rows)],\n",
    "    \"Ad_Type\": [random.choice(ad_types) for _ in range(rows)],\n",
    "    \"Target_Region\": [random.choice(['NA', 'EMEA', 'APAC', 'LATAM', 'Global']) for _ in range(rows)],\n",
    "    \n",
    "    # Messy URL with parameters (Great for Regex testing)\n",
    "    \"Landing_URL\": [f\"https://mysite.com/shop?utm_source={random.choice(platforms).lower()}&utm_id={fake.uuid4()[:5]}\" for _ in range(rows)],\n",
    "    \n",
    "    # JSON-like string (Simulating raw API data)\n",
    "    \"Ad_Parameters\": [str({'target_age': f\"{random.randint(18,60)}+\", 'bidding_strategy': 'auto'}) for _ in range(rows)],\n",
    "    # -------------------\n",
    "\n",
    "    \"Impressions\": [random.randint(1000, 50000) for _ in range(rows)],\n",
    "    \"Clicks\": [random.randint(10, 5000) for _ in range(rows)],\n",
    "    \"Conversions\": [random.randint(0, 500) for _ in range(rows)],\n",
    "    \"Spend\": [round(random.uniform(100, 10000), 2) for _ in range(rows)]\n",
    "})\n",
    "\n",
    "# --- LOGIC & CHAOS UPDATES ---\n",
    "\n",
    "# 1. Impossible Funnel: Clicks > Impressions (CTR > 100%)\n",
    "bad_ctr_mask = df_mkt.sample(frac=0.1).index\n",
    "df_mkt.loc[bad_ctr_mask, \"Clicks\"] = df_mkt.loc[bad_ctr_mask, \"Impressions\"] * 2\n",
    "\n",
    "# 2. Impossible Funnel: Conversions > Clicks (Conversion Rate > 100%)\n",
    "# This implies users bought the product without clicking the ad\n",
    "bad_cvr_mask = df_mkt.sample(frac=0.1).index\n",
    "df_mkt.loc[bad_cvr_mask, \"Conversions\"] = df_mkt.loc[bad_cvr_mask, \"Clicks\"] + 50\n",
    "\n",
    "# 3. Negative Spend (Billing error)\n",
    "df_mkt.loc[0:20, \"Spend\"] = [random.randint(-500, -1) for _ in range(21)]\n",
    "\n",
    "# 4. JSON Corruption (Broken Syntax)\n",
    "# We cast to object first to be safe\n",
    "df_mkt = df_mkt.astype(object)\n",
    "df_mkt.loc[21:30, \"Ad_Parameters\"] = \"{'target_age': '18-24', 'bidding': 'aut\" # Cut off string\n",
    "\n",
    "# 5. Broken URLs\n",
    "df_mkt.loc[31:40, \"Landing_URL\"] = \"http://site.com/??error=true\"\n",
    "\n",
    "# 6. Apply Generic Chaos\n",
    "df_mkt = apply_generic_chaos(df_mkt)\n",
    "\n",
    "df_mkt.to_csv(f\"{OUTPUT_DIR}/Marketing_dataset.csv\", index=False)\n",
    "print(\"Marketing Dataset generated and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18dbd0a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating HR Dataset with 10000 rows.\n",
      "HR Dataset generated and saved.\n"
     ]
    }
   ],
   "source": [
    "# HR Dataset\n",
    "print(f\"Generating HR Dataset with {rows} rows.\")\n",
    "\n",
    "depts = ['Engineering', 'Sales', 'HR', 'Finance', 'Legal', 'Operations']\n",
    "titles = ['Intern', 'Associate', 'Manager', 'Director', 'VP', 'C-Level']\n",
    "\n",
    "df_hr = pd.DataFrame({\n",
    "    \"Emp_ID\": [f\"E-{random.randint(1000, 9999)}\" for _ in range(rows)],\n",
    "    \"Name\": [fake.name() for _ in range(rows)],\n",
    "    \n",
    "    # --- NEW COLUMNS ---\n",
    "    \"Email\": [fake.company_email() for _ in range(rows)],\n",
    "    \"Department\": [random.choice(depts) for _ in range(rows)],\n",
    "    \"Title\": [random.choice(titles) for _ in range(rows)],\n",
    "    \"Salary\": [random.randint(30000, 250000) for _ in range(rows)],\n",
    "    \"Performance_Score\": [random.choice([1, 2, 3, 4, 5, np.nan]) for _ in range(rows)],\n",
    "    # -------------------\n",
    "\n",
    "    \"Age\": [random.randint(18, 65) for _ in range(rows)],\n",
    "    \"Marital_Status\": [random.choice(['Single', 'Married', 'Divorced', 'Widowed']) for _ in range(rows)],\n",
    "    \n",
    "    \"Join_Date\": [fake.date_this_year() for _ in range(rows)],\n",
    "    \"Exit_Date\": [random.choice([fake.date_this_year(), np.nan, np.nan, np.nan]) for _ in range(rows)]\n",
    "})\n",
    "\n",
    "# --- LOGIC & CHAOS UPDATES ---\n",
    "\n",
    "# 1. Child Labor & Child Marriage (Your original logic)\n",
    "# 5-year-olds who are married\n",
    "bad_hr_indices = df_hr.sample(20).index\n",
    "df_hr.loc[bad_hr_indices, \"Age\"] = 5\n",
    "df_hr.loc[bad_hr_indices, \"Marital_Status\"] = \"Married\" \n",
    "\n",
    "# 2. Time Travel (Exit Date BEFORE Join Date)\n",
    "# Find people who have an exit date\n",
    "has_exit = df_hr[df_hr[\"Exit_Date\"].notna()].index\n",
    "# Take the first 10 of them and make them leave before they joined\n",
    "df_hr.loc[has_exit[:10], \"Exit_Date\"] = df_hr.loc[has_exit[:10], \"Join_Date\"] - datetime.timedelta(days=100)\n",
    "\n",
    "# 3. Future Hires (Join Date in 2099)\n",
    "df_hr.loc[0:10, \"Join_Date\"] = datetime.date(2099, 1, 1)\n",
    "\n",
    "# 4. Salary Formatting Chaos (Strings in Integer column)\n",
    "# Cast to object first so we can mix strings and ints\n",
    "df_hr = df_hr.astype(object) \n",
    "df_hr.loc[11:20, \"Salary\"] = \"$100,000\" # Currency formatting\n",
    "df_hr.loc[21:30, \"Salary\"] = \"TBD\"      # Text placeholder\n",
    "\n",
    "# 5. Invalid Emails (Missing @ symbol)\n",
    "df_hr.loc[31:40, \"Email\"] = \"user_company.com\"\n",
    "\n",
    "# 6. Apply Generic Chaos\n",
    "df_hr = apply_generic_chaos(df_hr)\n",
    "\n",
    "df_hr.to_csv(f\"{OUTPUT_DIR}/HR_dataset.csv\", index=False)\n",
    "print(\"HR Dataset generated and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2467e02b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Logistics Dataset with 10000 rows.\n",
      "Logistics Dataset generated and saved.\n"
     ]
    }
   ],
   "source": [
    "# Logistics Dataset\n",
    "print(f\"Generating Logistics Dataset with {rows} rows.\")\n",
    "\n",
    "vehicles = ['Semi-Truck', 'Van', 'Pickup', 'Drone', 'Cargo Ship']\n",
    "statuses = ['In Transit', 'Idling', 'Maintenance', 'Delivered', 'Accident']\n",
    "\n",
    "df_log = pd.DataFrame({\n",
    "    \"Trip_ID\": [f\"TR-{fake.uuid4()[:8]}\" for _ in range(rows)],\n",
    "    \"Driver_Name\": [fake.name() for _ in range(rows)],\n",
    "    \"Vehicle_Type\": [random.choice(vehicles) for _ in range(rows)],\n",
    "    \n",
    "    # --- NEW COLUMNS ---\n",
    "    \"Origin\": [fake.city() for _ in range(rows)],\n",
    "    \"Destination\": [fake.city() for _ in range(rows)],\n",
    "    \n",
    "    # Coordinates: formatted as \"Lat, Long\" string for parsing tests\n",
    "    \"GPS_Coordinates\": [f\"{fake.latitude()}, {fake.longitude()}\" for _ in range(rows)],\n",
    "    \n",
    "    \"Distance_Miles\": [random.randint(0, 3000) for _ in range(rows)],\n",
    "    \"Fuel_Gallons\": [random.uniform(0, 500) for _ in range(rows)],\n",
    "    \"Status\": [random.choice(statuses) for _ in range(rows)]\n",
    "})\n",
    "\n",
    "# --- LOGIC & CHAOS UPDATES ---\n",
    "\n",
    "# 1. Teleportation (Distance is 0, but Origin != Destination)\n",
    "# This breaks logic checks that assume Dist=0 implies same location\n",
    "teleport_mask = df_log.sample(frac=0.05).index\n",
    "df_log.loc[teleport_mask, \"Distance_Miles\"] = 0\n",
    "\n",
    "# 2. Infinite Efficiency (Distance > 0, but Fuel = 0)\n",
    "# This causes Division By Zero errors if you calculate MPG (Miles Per Gallon)\n",
    "magic_truck_mask = df_log.sample(frac=0.05).index\n",
    "df_log.loc[magic_truck_mask, \"Distance_Miles\"] = 500\n",
    "df_log.loc[magic_truck_mask, \"Fuel_Gallons\"] = 0\n",
    "\n",
    "# 3. Circular Trips (Origin == Destination)\n",
    "# Legitimate in some cases, but often a data error for long-haul\n",
    "df_log.loc[0:20, \"Origin\"] = df_log.loc[0:20, \"Destination\"]\n",
    "\n",
    "# 4. Impossible Coordinates (Lat > 90)\n",
    "# This will break mapping software like Tableau or PowerBI\n",
    "df_log.loc[21:30, \"GPS_Coordinates\"] = \"99.9999, 150.0000\"\n",
    "\n",
    "# 5. Data Type Chaos (String in Numeric Column)\n",
    "# Convert to object first to handle the string injection\n",
    "df_log = df_log.astype(object)\n",
    "df_log.loc[31:40, \"Distance_Miles\"] = \"Unknown\" \n",
    "df_log.loc[41:50, \"Fuel_Gallons\"] = -50.0 # Negative fuel (Production of fuel?)\n",
    "\n",
    "# 6. Apply Generic Chaos\n",
    "df_log = apply_generic_chaos(df_log)\n",
    "\n",
    "df_log.to_csv(f\"{OUTPUT_DIR}/Logistics_dataset.csv\", index=False)\n",
    "print(\"Logistics Dataset generated and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ecf54d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Customer Service Dataset with 10000 rows.\n",
      "Customer Service Dataset generated and saved.\n"
     ]
    }
   ],
   "source": [
    "# Customer Service Dataset\n",
    "print(f\"Generating Customer Service Dataset with {rows} rows.\")\n",
    "\n",
    "topics = ['Billing', 'Technical Support', 'Feature Request', 'Login Issue', 'Refund']\n",
    "channels = ['Email', 'Chat', 'Phone', 'Social Media', 'Pigeon Carrier']\n",
    "statuses = ['Open', 'In Progress', 'Resolved', 'Closed', 'Escalated']\n",
    "\n",
    "df_cs = pd.DataFrame({\n",
    "    \"Ticket_ID\": [f\"TKT-{random.randint(10000, 99999)}\" for _ in range(rows)],\n",
    "    \"Customer_Email\": [fake.email() for _ in range(rows)],\n",
    "    \n",
    "    # --- NEW COLUMNS ---\n",
    "    \"Customer_Name\": [fake.name() for _ in range(rows)],\n",
    "    \"Topic\": [random.choice(topics) for _ in range(rows)],\n",
    "    \"Channel\": [random.choice(channels) for _ in range(rows)],\n",
    "    \"Agent_Name\": [random.choice([fake.first_name(), \"Bot\", np.nan]) for _ in range(rows)],\n",
    "    \n",
    "    # CSAT Score: Should be 1-5, but we'll break this rule later\n",
    "    \"CSAT_Score\": [random.randint(1, 5) for _ in range(rows)],\n",
    "    \n",
    "    # Priority: Intentionally inconsistent (High, H, 1, Urgent)\n",
    "    \"Priority\": [random.choice(['High', 'H', 'Medium', 'Med', 'Low', 'L', 'Urgent']) for _ in range(rows)],\n",
    "    # -------------------\n",
    "\n",
    "    \"Created_Date\": [fake.date_this_year() for _ in range(rows)],\n",
    "    \"Resolved_Date\": [fake.date_this_year() for _ in range(rows)],\n",
    "    \"Resolution_Hours\": [round(random.uniform(0.5, 72.0), 2) for _ in range(rows)],\n",
    "    \"Status\": [random.choice(statuses) for _ in range(rows)]\n",
    "})\n",
    "\n",
    "# --- LOGIC & CHAOS UPDATES ---\n",
    "\n",
    "# 1. Time Travel Resolution (Resolved Date BEFORE Created Date)\n",
    "# A classic database error that messes up SLA calculations\n",
    "time_travel_mask = df_cs.sample(frac=0.05).index\n",
    "df_cs.loc[time_travel_mask, \"Resolved_Date\"] = df_cs.loc[time_travel_mask, \"Created_Date\"] - datetime.timedelta(days=2)\n",
    "\n",
    "# 2. Negative Resolution Time (Finished before starting)\n",
    "df_cs.loc[0:20, \"Resolution_Hours\"] = [random.randint(-50, -1) for _ in range(21)]\n",
    "\n",
    "# 3. Ghost Resolutions (Status = Closed, but Agent is Null)\n",
    "# Implies the ticket closed itself?\n",
    "ghost_mask = df_cs.sample(frac=0.05).index\n",
    "df_cs.loc[ghost_mask, \"Status\"] = \"Closed\"\n",
    "df_cs.loc[ghost_mask, \"Agent_Name\"] = np.nan\n",
    "\n",
    "# 4. CSAT Out of Bounds (Score of 10 on a 5-point scale)\n",
    "df_cs.loc[21:30, \"CSAT_Score\"] = 10 \n",
    "df_cs.loc[31:40, \"CSAT_Score\"] = 0 \n",
    "\n",
    "# 5. Data Type Chaos (Text in CSAT Score)\n",
    "# Cast to object first so we can mix text into the number column\n",
    "df_cs = df_cs.astype(object)\n",
    "df_cs.loc[41:50, \"CSAT_Score\"] = \"Happy\" \n",
    "df_cs.loc[51:60, \"CSAT_Score\"] = \"Angry\"\n",
    "\n",
    "# 6. Apply Generic Chaos\n",
    "df_cs = apply_generic_chaos(df_cs)\n",
    "\n",
    "df_cs.to_csv(f\"{OUTPUT_DIR}/Customer_service_dataset.csv\", index=False)\n",
    "print(\"Customer Service Dataset generated and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7436543a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Insurance Dataset with 10000 rows.\n",
      "Insurance Dataset generated and saved.\n"
     ]
    }
   ],
   "source": [
    "# Insurance Dataset\n",
    "print(f\"Generating Insurance Dataset with {rows} rows.\")\n",
    "\n",
    "policy_types = ['Auto', 'Home', 'Life', 'Health', 'Cyber', 'Pet']\n",
    "statuses = ['Approved', 'Denied', 'Under Investigation', 'Pending', 'Fraud Alert']\n",
    "\n",
    "df_ins = pd.DataFrame({\n",
    "    \"Policy_ID\": [f\"POL-{fake.uuid4()[:8]}\" for _ in range(rows)],\n",
    "    \"Policy_Holder\": [fake.name() for _ in range(rows)],\n",
    "    \"Policy_Type\": [random.choice(policy_types) for _ in range(rows)],\n",
    "    \n",
    "    # --- NEW COLUMNS ---\n",
    "    \"Start_Date\": [fake.date_this_year() for _ in range(rows)],\n",
    "    \"Incident_Date\": [fake.date_this_year() for _ in range(rows)],\n",
    "    \n",
    "    \"Premium_Monthly\": [round(random.uniform(50, 500), 2) for _ in range(rows)],\n",
    "    \"Deductible\": [random.choice([500, 1000, 2500, 5000]) for _ in range(rows)],\n",
    "    \"Risk_Score\": [random.randint(1, 100) for _ in range(rows)],\n",
    "    # -------------------\n",
    "\n",
    "    \"Coverage_Limit\": [random.choice([10000, 50000, 100000, 500000, 1000000]) for _ in range(rows)],\n",
    "    \"Claim_Amount\": [round(random.uniform(100, 50000), 2) for _ in range(rows)],\n",
    "    \"Status\": [random.choice(statuses) for _ in range(rows)]\n",
    "})\n",
    "\n",
    "# --- LOGIC & CHAOS UPDATES ---\n",
    "\n",
    "# 1. Catastrophic Claims (Claim > Coverage)\n",
    "# Useful for testing logic that should flag \"Total Loss\" or \"Limit Reached\"\n",
    "bad_claim_mask = df_ins.sample(frac=0.05).index\n",
    "df_ins.loc[bad_claim_mask, \"Claim_Amount\"] = df_ins.loc[bad_claim_mask, \"Coverage_Limit\"] * 1.5\n",
    "\n",
    "# 2. Pre-existing Conditions (Incident Date BEFORE Start Date)\n",
    "# A classic fraud indicator\n",
    "fraud_mask = df_ins.sample(frac=0.05).index\n",
    "df_ins.loc[fraud_mask, \"Incident_Date\"] = df_ins.loc[fraud_mask, \"Start_Date\"] - datetime.timedelta(days=30)\n",
    "\n",
    "# 3. Negative Premiums (Logic Error)\n",
    "df_ins.loc[0:20, \"Premium_Monthly\"] = -150.00\n",
    "\n",
    "# 4. Data Type Chaos (String in Numeric Column)\n",
    "# Cast to object first so we can inject text\n",
    "df_ins = df_ins.astype(object)\n",
    "\n",
    "# Inject \"Unlimited\" into Coverage Limit (Breaks math operations)\n",
    "df_ins.loc[21:30, \"Coverage_Limit\"] = \"Unlimited\"\n",
    "\n",
    "# Inject \"Waived\" into Deductible\n",
    "df_ins.loc[31:40, \"Deductible\"] = \"Waived\"\n",
    "\n",
    "# 5. Apply Generic Chaos\n",
    "df_ins = apply_generic_chaos(df_ins)\n",
    "\n",
    "df_ins.to_csv(f\"{OUTPUT_DIR}/Insurance_dataset.csv\", index=False)\n",
    "print(\"Insurance Dataset generated and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7dd71442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Real Estate Dataset with 10000 rows.\n",
      "Real Estate Dataset generated and saved.\n"
     ]
    }
   ],
   "source": [
    "# Real Estate Dataset\n",
    "print(f\"Generating Real Estate Dataset with {rows} rows.\")\n",
    "\n",
    "prop_types = ['Single Family', 'Condo', 'Townhouse', 'Multi-Family', 'Land']\n",
    "status_list = ['Active', 'Sold', 'Pending', 'Withdrawn', 'Foreclosure']\n",
    "\n",
    "df_re = pd.DataFrame({\n",
    "    \"Property_ID\": [f\"MLS-{random.randint(100000, 999999)}\" for _ in range(rows)],\n",
    "    \"Address\": [fake.street_address() for _ in range(rows)],\n",
    "    \"City\": [fake.city() for _ in range(rows)],\n",
    "    \"Zip_Code\": [fake.zipcode() for _ in range(rows)],\n",
    "    \n",
    "    # --- NEW COLUMNS ---\n",
    "    \"Prop_Type\": [random.choice(prop_types) for _ in range(rows)],\n",
    "    \"Year_Built\": [random.randint(1900, 2024) for _ in range(rows)],\n",
    "    \"Bathrooms\": [random.choice([1, 1.5, 2, 2.5, 3, 4, 5]) for _ in range(rows)],\n",
    "    \"Garage\": [random.choice([True, False, \"Yes\", \"No\", 0, 1]) for _ in range(rows)], # Mixed Boolean\n",
    "    # -------------------\n",
    "\n",
    "    \"Price\": [random.randint(50000, 2000000) for _ in range(rows)],\n",
    "    \"Bedrooms\": [random.randint(1, 6) for _ in range(rows)],\n",
    "    \"SqFt\": [random.randint(500, 10000) for _ in range(rows)],\n",
    "    \n",
    "    \"Listed_Date\": [fake.date_this_year() for _ in range(rows)],\n",
    "    \"Sold_Date\": [random.choice([fake.date_this_year(), np.nan]) for _ in range(rows)],\n",
    "    \"Status\": [random.choice(status_list) for _ in range(rows)]\n",
    "})\n",
    "\n",
    "# --- LOGIC & CHAOS UPDATES ---\n",
    "\n",
    "# 1. Physics Violation (The \"Shoebox Mansion\")\n",
    "# 100 Bedrooms inside 500 SqFt (Impossible density)\n",
    "impossible_mask = df_re.sample(frac=0.05).index\n",
    "df_re.loc[impossible_mask, \"Bedrooms\"] = 100\n",
    "df_re.loc[impossible_mask, \"SqFt\"] = 500\n",
    "\n",
    "# 2. Time Travel (Sold BEFORE Listed)\n",
    "sold_mask = df_re[df_re[\"Sold_Date\"].notna()].sample(frac=0.1).index\n",
    "df_re.loc[sold_mask, \"Sold_Date\"] = df_re.loc[sold_mask, \"Listed_Date\"] - datetime.timedelta(days=100)\n",
    "\n",
    "# 3. Future Buildings (Year Built 2099)\n",
    "df_re.loc[0:10, \"Year_Built\"] = 2099\n",
    "\n",
    "# 4. Negative SqFt (Dimensional error)\n",
    "df_re.loc[11:20, \"SqFt\"] = -1000\n",
    "\n",
    "# 5. Data Type Chaos (Strings in Price)\n",
    "# Cast to object to allow text injection\n",
    "df_re = df_re.astype(object)\n",
    "df_re.loc[21:30, \"Price\"] = \"Call for Price\"\n",
    "df_re.loc[31:40, \"Price\"] = \"$1M+\"\n",
    "\n",
    "# 6. Apply Generic Chaos\n",
    "df_re = apply_generic_chaos(df_re)\n",
    "\n",
    "df_re.to_csv(f\"{OUTPUT_DIR}/Real_estate_dataset.csv\", index=False)\n",
    "print(\"Real Estate Dataset generated and saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
